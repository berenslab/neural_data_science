{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Neural Data Analysis_\n",
    "\n",
    "Lecturer: Prof. Dr. Philipp Berens\n",
    "\n",
    "Tutors: Sarah Strauss, Ziwei Huang\n",
    "\n",
    "Summer term 2021\n",
    "\n",
    "Name: FILL IN YOUR NAMES HERE\n",
    "\n",
    "# Exercise sheet 6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "\n",
    "mpl.rc(\"savefig\", dpi=72)\n",
    "\n",
    "import itertools\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Implement entropy estimators\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General framework\n",
    "\n",
    "Entropy is defined as \n",
    "\n",
    "$$\n",
    "H[p] = -\\sum_x p_x \\log p_x\n",
    "$$\n",
    "\n",
    "where $p_x = p(x=X)$. Here we assume that $X$ is a discrete random variable and that there are finitely many states $K$ that $X$ can take.\n",
    "\n",
    "We are interested in the entropy of discrete random variables, because of its relationship with mutual information:\n",
    "\n",
    "$$\n",
    "I[X|Y] = H[X] - H[X|Y]\n",
    "$$\n",
    "\n",
    "If we can estimate the entropy well, we can estimate the mutual information well. An application in neuroscience would be estimating the mutual information between a spike train modeled as a sequence of $1$s and $0$s (e.g. $(0,1,0,1,1)$) and a discrete set of stimuli.\n",
    "\n",
    "Note that a multivariate binary distribution modeling a spike train can always be mapped to a discrete univariate distribution, $\\mathbb{Z}_2 \\longrightarrow \\mathbb{Z_+}$, by interpreting each binary state $z \\in \\mathbb{Z}_2$ as its corresponding binary number and computing $f(z) = \\sum_i 2^{i} z_i$.\n",
    "\n",
    "As discussed in the lecture, the problem is that one always underestimates the true entropy of a distribution from samples. In this exercise you are meant to implement different estimators for discrete entropy and evaluate them on different discrete distributions:\n",
    "\n",
    "* Uniform distribution: $p(x=X) = \\frac{1}{K}$\n",
    "\n",
    "* \"Zipf's law\"- distribution: $p(x=X) = \\frac{1}{Z x} $, where $Z = \\sum_k 1/k$\n",
    "\n",
    "There is a really good series of blog posts about discrete entropy estimation to be found [here](http://www.nowozin.net/sebastian/blog/estimating-discrete-entropy-part-1.html), [here](http://www.nowozin.net/sebastian/blog/estimating-discrete-entropy-part-2.html) and [here](http://www.nowozin.net/sebastian/blog/estimating-discrete-entropy-part-3.html). \n",
    "\n",
    "Make sure you use binary logarithms throughout.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the estimators\n",
    "\n",
    "Implement the\n",
    "\n",
    "* maximum likelihood estimator (1 pt)\n",
    "* miller-maddow corrected estimator (1 pt)\n",
    "* jack-knife corrected estimator (2 pt)\n",
    "* coverage adjusted estimator (1 pt).\n",
    "\n",
    "When implementing the jack-knife estimator, you may want to restrict the amount of resampling for performance reasons e.g. to 1000, even if more samples are available. By definition, $0\\log0=0$. Adapt the interfaces as needed for your implementation.\n",
    "\n",
    "In addition, implement or use one of the following more advanced estimators (1+3 pts, extra points if you use your own implementation):\n",
    "\n",
    "* [JVHW estimator](https://arxiv.org/abs/1406.6956) with code on [github](https://github.com/EEthinker/JVHW_Entropy_Estimators/tree/master/Python)\n",
    "* [Unseen estimator](http://papers.nips.cc/paper/5170-estimating-the-unseen-improved-estimators-for-entropy-and-other-properties) (includes Matlab code in Supplementary)\n",
    "* [Best Upper Bounds estimator](http://www.mitpressjournals.org/doi/abs/10.1162/089976603321780272) (Matlab code available on Ilias)\n",
    "\n",
    "For this part, you are allowed to use an existing implementation as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_mle(phat):\n",
    "    '''\n",
    "    Maximum likelihood or plug-in estimator of discrete entropy\n",
    "    \n",
    "    phat:    estimate of the distribution / histogram\n",
    "    H:       entropy estimate\n",
    "    '''\n",
    "    \n",
    "\n",
    "    \n",
    "    return H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_mm(phat, n):\n",
    "    '''\n",
    "    Miller-Maddow corrected estimator of discrete entropy\n",
    "    \n",
    "    phat:    estimate of the distribution / histogram\n",
    "    n:       sample size\n",
    "    H:       entropy estimate\n",
    "    '''\n",
    "    \n",
    "\n",
    "    \n",
    "    return H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_jk(x, edges):\n",
    "    '''\n",
    "    Jack-knife corrected estimator of discrete entropy\n",
    "    \n",
    "\n",
    "    '''\n",
    "\n",
    "    \n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy_cae(phat, n):\n",
    "    '''\n",
    "    coverage-adjusted estimator of discrete entropy\n",
    "    \n",
    "    phat:    estimate of the distribution / histogram\n",
    "    n:       sample size\n",
    "    H:       entropy estimate\n",
    "    '''\n",
    "    \n",
    "\n",
    "    \n",
    "    return H\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import est_entro as ee\n",
    "\n",
    "def entropy_jvwh(x):\n",
    "    '''\n",
    "    JVHW estimator of discrete entropy\n",
    "    \n",
    "\n",
    "    '''\n",
    "\n",
    "    \n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 10\n",
    "N = 2**D\n",
    "\n",
    "p = 1/N * np.ones(N)   # true distribution\n",
    "\n",
    "H = - np.sum(p * np.log2(p))  # true entropy\n",
    "\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample from the uniform distribution using sample sizes of 100 and 10000. Plot the true distribution and the sampled distributions. What do you notice? (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the framework below to generate samples of different size (logarithmically spaced between 10 and 100000) and evaluate the different entropy estimators for multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSz = np.round(np.logspace(1,5,num=10))\n",
    "nRuns = 30\n",
    "\n",
    "edges = np.arange(-0.5, N, 1)\n",
    "\n",
    "h_mle = np.zeros((len(sampleSz),nRuns))\n",
    "h_mm = np.zeros((len(sampleSz),nRuns))\n",
    "h_jk = np.zeros((len(sampleSz),nRuns))\n",
    "h_cae = np.zeros((len(sampleSz),nRuns))\n",
    "h_jvhw = np.zeros((len(sampleSz),nRuns))\n",
    "\n",
    "for i, S in enumerate(sampleSz):\n",
    "    for j in np.arange(nRuns):\n",
    "        \n",
    "        # add sampling here...        \n",
    "        \n",
    "        phat = np.histogram(x,edges)[0]\n",
    "        phat = phat / S\n",
    "        \n",
    "        h_mle[i,j] = entropy_mle(phat)\n",
    "        h_mm[i,j] = entropy_mm(phat, S)\n",
    "        h_cae[i,j] = entropy_cae(phat, S)\n",
    "        h_jk[i,j] = entropy_jk(x, edges)\n",
    "        \n",
    "        h_jvhw[i,j] = entropy_jvwh(x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the resulting average estimate of the entropy for each of the estimators. Which is best? If you implemented everything correctly, this plot should roughly look like in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(sampleSz,np.mean(h_mle, axis=1))\n",
    "\n",
    "plt.semilogx(sampleSz,np.mean(h_mm, axis=1))\n",
    "plt.semilogx(sampleSz, np.mean(h_cae, axis=1))\n",
    "plt.semilogx(sampleSz,np.mean(h_jk, axis=1))\n",
    "plt.semilogx(sampleSz,np.mean(h_jvhw, axis=1))\n",
    "\n",
    "\n",
    "plt.legend(['mle', 'mm',  'cae', 'jk', 'jvhw'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zipf distribution\n",
    "\n",
    "[Zipf's law ](https://en.wikipedia.org/wiki/Zipf%27s_law) refers to a family of power law like distributions for which $p_k \\sim 1/k^d$. We will simply use $d=1$ here.   \n",
    "\n",
    "Adapt the framework above to sample from a Zipf distribution and evaluate the estimators for this case. Are there differences to the uniform case? (3 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 10\n",
    "N = 2**D\n",
    "\n",
    "p = 1/(np.arange(0,N)+1)    # true distribution\n",
    "p = p/np.sum(p)\n",
    "\n",
    "H = - np.sum(p * np.log2(p))  # true entropy\n",
    "\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample from the Zipf distribution using sample sizes of 100 and 10000. In this case, the function `random.choice` is very helpful for sampling. Plot the true distribution and the sampled distributions. What do you notice? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the framework below to generate samples of different size (logarithmically spaced between 10 and 100000) and evaluate the different entropy estimators for multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSz = np.round(np.logspace(1,5,num=10))\n",
    "nRuns = 30\n",
    "\n",
    "edges = np.arange(-0.5, N, 1)\n",
    "\n",
    "h_mle = np.zeros((len(sampleSz),nRuns))\n",
    "h_mm = np.zeros((len(sampleSz),nRuns))\n",
    "h_jk = np.zeros((len(sampleSz),nRuns))\n",
    "h_cae = np.zeros((len(sampleSz),nRuns))\n",
    "h_jvhw = np.zeros((len(sampleSz),nRuns))\n",
    "\n",
    "for i, S in enumerate(sampleSz):\n",
    "    for j in np.arange(nRuns):\n",
    "        \n",
    "        # add sampling here\n",
    "        \n",
    "        phat = np.histogram(x,edges)[0]\n",
    "        phat = phat / S\n",
    "        \n",
    "        h_mle[i,j] = entropy_mle(phat)\n",
    "        h_mm[i,j] = entropy_mm(phat, S)\n",
    "        h_cae[i,j] = entropy_cae(phat, S)\n",
    "        h_jk[i,j] = entropy_jk(x, edges)\n",
    "        \n",
    "        h_jvhw[i,j] = entropy_jvwh(x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot resulting average estimate of the entropy for each of the estimators. Which is best? If you implemented everything correctly, this plot should roughly look like in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.semilogx(sampleSz,np.mean(h_mle, axis=1))\n",
    "\n",
    "plt.semilogx(sampleSz,np.mean(h_mm, axis=1))\n",
    "plt.semilogx(sampleSz, np.mean(h_cae, axis=1))\n",
    "plt.semilogx(sampleSz,np.mean(h_jk, axis=1))\n",
    "plt.semilogx(sampleSz,np.mean(h_jvhw, axis=1))\n",
    "\n",
    "\n",
    "plt.legend(['mle', 'mm',  'cae', 'jk', 'jvhw'])\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.13.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
